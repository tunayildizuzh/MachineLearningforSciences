{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8713509a",
   "metadata": {},
   "source": [
    "# Lab-8: Deep Learning with Lightning\n",
    "\n",
    "This lab is design for investigating the basic architecture and parameter searching techniques in deep learning literature. We use a fundamental dataset, MNIST, with the help of PyTorch Lightning and design our model to handle image classification task.\n",
    "\n",
    "### General Announcements\n",
    "\n",
    "* The exercises on this sheet are graded by a maximum of **14 points**. You will be asked to implement several functions.\n",
    "* Team work is not allowed! Everybody implements his/her own code. Discussing issues with others is fine, sharing code with others is not. \n",
    "* If you use any code fragments found on the Internet, make sure you reference them properly.\n",
    "* You can send your questions via email to the TAs until the deadline.\n",
    "\n",
    "### Suggestions\n",
    "\n",
    "Please install pytorch lightning packages via conda or pip before starting the lab session. You can use the tutorials of [Pytorch Lightning](https://lightning.ai/docs/pytorch/stable/notebooks/lightning_examples/mnist-hello-world.html).\n",
    "\n",
    "Please also check: [Tensorboard](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html)\n",
    "\n",
    "For installing lightning: [Link](https://lightning.ai/pytorch-lightning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Machine Learning Modules\n",
    "import pandas\n",
    "import numpy\n",
    "import sklearn\n",
    "\n",
    "# Deep Learning Modules\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "# Visualization Modules\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Others\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torchmetrics import Accuracy\n",
    "from lightning.pytorch.loggers import CSVLogger, TensorBoardLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "import torchmetrics\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from tqdm.auto import tqdm\n",
    "from pytorch_lightning.callbacks import EarlyStopping, PlotLossesCallback\n",
    "\n",
    "DATASET_PATH = r'C:\\Users\\tunay\\OneDrive\\Desktop\\github_repos\\MachineLearningforSciences\\Datasets'  # You can change them\n",
    "EXPERIMENTS_PATH = r'C:\\Users\\tunay\\OneDrive\\Desktop\\github_repos\\MachineLearningforSciences\\Datasets'  # You can change them"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "435db621",
   "metadata": {},
   "source": [
    "# 1) Dataset (3 Points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d2af472",
   "metadata": {},
   "source": [
    "- Design a DataModule for MNIST.\n",
    "- Use 80/20 % split for train/val sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2260644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs: 20\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "print(\"Number of CPUs:\", num_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea48159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]]), tensor([2, 2, 1, 0, 6, 0, 0, 8, 2, 0, 9, 4, 8, 5, 7, 6, 1, 8, 0, 4, 0, 8, 3, 5,\n",
      "        1, 3, 8, 3, 8, 6, 8, 7, 7, 5, 0, 4, 3, 8, 2, 6, 4, 7, 4, 0, 5, 7, 4, 6,\n",
      "        6, 9, 2, 3, 0, 3, 2, 6, 5, 8, 2, 3, 7, 7, 6, 6])]\n"
     ]
    }
   ],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_folder: str = DATASET_PATH, batch_size: int = 64, num_cpu: int = 1):\n",
    "        super().__init__()\n",
    "        self.path = data_folder\n",
    "        self.batch_size = batch_size\n",
    "        self.num_cpu = num_cpu\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    \n",
    "    def prepare_data(self) -> None:\n",
    "        # Download MNIST Data in self.path\n",
    "\n",
    "        MNIST(self.path, train=True, download=True)\n",
    "        MNIST(self.path, train=False, download=True)\n",
    "    \n",
    "    def setup(self, stage: str = 'fit') -> None:\n",
    "        if stage in ['fit', 'tune']:\n",
    "            self.train_dataset = MNIST(self.path, train=True, transform=self.transform)\n",
    "        \n",
    "        if stage in ['fit', 'tune', 'validate']:\n",
    "            self.val_dataset = MNIST(self.path, train=False, transform=self.transform) \n",
    "        \n",
    "        elif stage in ['test', 'predict']:\n",
    "            self.test_dataset = MNIST(self.path, train=False, transform=self.transform) \n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError('Unknown Stage: {}'.format(stage))\n",
    "            \n",
    "    def train_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        return torch.utils.data.DataLoader(\n",
    "            batch_size=self.batch_size, num_workers=self.num_cpu,  # DO NOT CHANGE IN VAL/TEST LOADERS\n",
    "            dataset=self.train_dataset, shuffle=True,  # Could be changed in val/test loaders\n",
    "        )\n",
    "    def val_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        # Insert your code\n",
    "        return torch.utils.data.DataLoader(\n",
    "            batch_size=self.batch_size, num_workers=self.num_cpu,  # DO NOT CHANGE IN VAL/TEST LOADERS\n",
    "            dataset=self.val_dataset, shuffle=False,  # Could be changed in val/test loaders\n",
    "        )\n",
    "\n",
    "    \n",
    "    def test_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        # Insert your code\n",
    "        return torch.utils.data.DataLoader(\n",
    "            batch_size=self.batch_size, num_workers=self.num_cpu,  # DO NOT CHANGE IN VAL/TEST LOADERS\n",
    "            dataset=self.test_dataset, shuffle=False,  # Could be changed in val/test loaders\n",
    "        )\n",
    "    \n",
    "    def predict_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        return test_dataloader()\n",
    "\n",
    "data_module = MNISTDataModule(num_cpu=20)\n",
    "data_module.prepare_data()\n",
    "data_module.setup('fit')\n",
    "dl = data_module.train_dataloader()\n",
    "print(next(dl.__iter__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc5c915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Images: [B x C x H x W] =  torch.Size([64, 1, 28, 28])\n",
      "Shape of Labels: [B] =  torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Do not change! Only for checking\n",
    "print('Shape of Images: [B x C x H x W] = ', next(dl.__iter__())[0].shape)\n",
    "print('Shape of Labels: [B] = ', next(dl.__iter__())[1].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e47e49f-d888-4505-af63-2b3c516736a7",
   "metadata": {},
   "source": [
    "## 2) Neural Network Architecture (2 Points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf965eaa",
   "metadata": {},
   "source": [
    "- Design an model with 3 Convolutional layers and 1 fully-connected layer, in this order:\n",
    "    - Convolution: Kernel size = 3x3, padding = 'same', number of filters = 4\n",
    "    - Convolution: Kernel size = 3x3, padding = 'same', number of filters = 8\n",
    "    - Convolution: Kernel size = 3x3, padding = 'same', number of filters = 4\n",
    "    - Linear: No Bias, num_class = 10 in MNIST Dataset\n",
    "- Use rectified linear unit (ReLU) for activation function. \n",
    "- Initialize all the weights with `xavier_uniform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ccdcec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "class RawModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Insert your code\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, padding ='same')\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, padding ='same')\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=8, out_channels=4, kernel_size=3, padding ='same')\n",
    "        self.fc = torch.nn.Linear(in_features = 4*28*28, out_features=10, bias=False)\n",
    "        \n",
    "        self.apply(self.initialize_weights)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def initialize_weights(module: torch.nn.Module) -> None:\n",
    "        # Insert your code\n",
    "        if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):   #https://stackoverflow.com/questions/49433936/how-do-i-initialize-weights-in-pytorch\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "        \n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Input: torch.Tensor | dtype=torch.float | shape=[B, C, H, W]\n",
    "        Output: torch.Tensor | dtype=torch.float | shape=[B, num_class]\n",
    "        \"\"\"\n",
    "        # Insert your code\n",
    "        x = self.conv1(images)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = RawModel()\n",
    "with torch.no_grad():\n",
    "    sample_image = torch.rand(size=(4, 1, 28, 28))\n",
    "    output = model(sample_image)\n",
    "    print(output.shape, output.dtype)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9d0aef6-0e15-42b8-9bad-23093536ab66",
   "metadata": {},
   "source": [
    "## 3) Experiments (4 Points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9d4ef1e",
   "metadata": {},
   "source": [
    "- Define training/validation/test step and optimizer with cross-entropy loss and Adam optimizer.\n",
    "- Use accuracy scores for monitoring the experiment. (multiclass accuracy from Lightning Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30467cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.types import TRAIN_DATALOADERS\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class MNISTExperiment(pl.LightningModule):\n",
    "    def __init__(self, learning_rate: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.model = RawModel()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.train_scores = torchmetrics.Accuracy(compute_on_step=False, task='multiclass', num_classes=10)  # Placeholder for training scores\n",
    "        self.validation_scores = torchmetrics.Accuracy(compute_on_step=False, task='multiclass', num_classes=10)   # Placeholder for validation scores\n",
    "        self.test_scores = torchmetrics.Accuracy(compute_on_step=False, task='multiclass', num_classes=10)   # Placeholder for test scores\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        self.train_scores(preds, y)\n",
    "\n",
    "        self.log('train_accuracy', self.train_scores, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx) -> None:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        self.validation_scores(preds, y)\n",
    "\n",
    "    def test_step(self, batch, batch_idx) -> None:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        self.test_scores(preds, y)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset = MNIST('data/', train=True, download=True, transform=transform)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "        return dataloader\n",
    "\n",
    "experiment = MNISTExperiment()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67ca3037",
   "metadata": {},
   "source": [
    "- Define a trainer of lightning.\n",
    "    - Maximum epoch = 20\n",
    "    - accelerator = 'auto'\n",
    "    - Use `CSVLogger` and `TensorBoardLogger`\n",
    "    - Use `EarlyStopping` with patience epoch = 3\n",
    "    - Use `TQDMProgressBar` with refresh rate = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587290fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "plot_losses = PlotLossesCallback()\n",
    "\n",
    "# Define the loggers\n",
    "csv_logger = CSVLogger('logs', name='mnist_logs')\n",
    "tensorboard_logger = TensorBoardLogger('logs', name='mnist_logs')\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator='auto',\n",
    "    logger=[csv_logger, tensorboard_logger],\n",
    "    callbacks=[early_stopping, plot_losses]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6aab9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name              | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0 | model             | RawModel           | 32.0 K\n",
      "1 | train_scores      | MulticlassAccuracy | 0     \n",
      "2 | validation_scores | MulticlassAccuracy | 0     \n",
      "3 | test_scores       | MulticlassAccuracy | 0     \n",
      "---------------------------------------------------------\n",
      "32.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "32.0 K    Total params\n",
      "0.128     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d3dfb12ae0444ebb22c7f72f7441b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=20, accelerator='auto')\n",
    "trainer.fit(experiment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d8e166c",
   "metadata": {},
   "source": [
    "## 4) Results (4 Points)\n",
    "- Train and test the `RawModel` and plot the score and loss values versus epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ec009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bde4161",
   "metadata": {},
   "source": [
    "- Re-design the model with dropout layer in-between the convolutional layers and re-train the model. as like `RawModel` and create a new module as `ModelWithDropout`. Try:\n",
    "    - dropout probability = 0.1\n",
    "    - dropout probability = 0.5\n",
    "    - dropout probability = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65ec01f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Insert your code\n",
    "class ModelWithDropout(torch.nn.Module):\n",
    "    def __init__(self, dropout_prob):\n",
    "        super().__init__()\n",
    "        # Insert your code\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, padding ='same')\n",
    "        self.drop_out1 = torch.nn.Dropout(dropout_prob)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, padding ='same')\n",
    "        self.drop_out2 = torch.nn.Dropout(dropout_prob)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=8, out_channels=4, kernel_size=3, padding ='same')\n",
    "        self.drop_out3 = torch.nn.Dropout(dropout_prob)\n",
    "        self.fc = torch.nn.Linear(in_features = 4*28*28, out_features=10, bias=False)\n",
    "        \n",
    "        self.apply(self.initialize_weights)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def initialize_weights(module: torch.nn.Module) -> None:\n",
    "        # Insert your code\n",
    "        if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):   #https://stackoverflow.com/questions/49433936/how-do-i-initialize-weights-in-pytorch\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "        \n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Input: torch.Tensor | dtype=torch.float | shape=[B, C, H, W]\n",
    "        Output: torch.Tensor | dtype=torch.float | shape=[B, num_class]\n",
    "        \"\"\"\n",
    "        # Insert your code\n",
    "        x = self.conv1(images)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.drop_out1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.drop_out2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.drop_out3(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = RawModel()\n",
    "with torch.no_grad():\n",
    "    sample_image = torch.rand(size=(4, 1, 28, 28))\n",
    "    output = model(sample_image)\n",
    "    print(output.shape, output.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "612e490a",
   "metadata": {},
   "source": [
    "- Re-design the model with batchnorm layer in-between the convolutional layers and re-train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f322cf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "class BatchNormModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Insert your code\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, padding='same')\n",
    "        self.batch_norm1 = torch.nn.BatchNorm2d(4)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, padding='same')\n",
    "        self.batch_norm2 = torch.nn.BatchNorm2d(8)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=8, out_channels=4, kernel_size=3, padding='same')\n",
    "        self.batch_norm3 = torch.nn.BatchNorm2d(4)\n",
    "        self.fc = torch.nn.Linear(in_features=4 * 28 * 28, out_features=10, bias=False)\n",
    "        \n",
    "        self.apply(self.initialize_weights)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def initialize_weights(module: torch.nn.Module) -> None:\n",
    "        # Insert your code\n",
    "        if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):   #https://stackoverflow.com/questions/49433936/how-do-i-initialize-weights-in-pytorch\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "        \n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Input: torch.Tensor | dtype=torch.float | shape=[B, C, H, W]\n",
    "        Output: torch.Tensor | dtype=torch.float | shape=[B, num_class]\n",
    "        \"\"\"\n",
    "        # Insert your code\n",
    "        x = self.conv1(images)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = RawModel()\n",
    "with torch.no_grad():\n",
    "    sample_image = torch.rand(size=(4, 1, 28, 28))\n",
    "    output = model(sample_image)\n",
    "    print(output.shape, output.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL WITH BATCH NORMALIZATION TRAINING\n",
    "\n",
    "class MNISTExperiment(pl.LightningModule):\n",
    "    def __init__(self, learning_rate: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.model = BatchNormModel()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.train_scores = torchmetrics.Accuracy(compute_on_step=False, task='multiclass', num_classes=10)  # Placeholder for training scores\n",
    "        self.validation_scores = torchmetrics.Accuracy(compute_on_step=False, task='multiclass', num_classes=10)   # Placeholder for validation scores\n",
    "        self.test_scores = torchmetrics.Accuracy(compute_on_step=False, task='multiclass', num_classes=10)   # Placeholder for test scores\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        self.train_scores(preds, y)\n",
    "\n",
    "        self.log('train_accuracy', self.train_scores, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx) -> None:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        self.validation_scores(preds, y)\n",
    "\n",
    "    def test_step(self, batch, batch_idx) -> None:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        self.test_scores(preds, y)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset = MNIST('data/', train=True, download=True, transform=transform)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "        return dataloader\n",
    "\n",
    "experiment2 = MNISTExperiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d115a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses = PlotLossesCallback()\n",
    "\n",
    "# Define the loggers\n",
    "csv_logger = CSVLogger('logs', name='mnist_logs')\n",
    "tensorboard_logger = TensorBoardLogger('logs', name='mnist_logs')\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator='auto',\n",
    "    logger=[csv_logger, tensorboard_logger],\n",
    "    callbacks=[early_stopping, plot_losses]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "154b4a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name              | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0 | model             | RawModel           | 32.0 K\n",
      "1 | train_scores      | MulticlassAccuracy | 0     \n",
      "2 | validation_scores | MulticlassAccuracy | 0     \n",
      "3 | test_scores       | MulticlassAccuracy | 0     \n",
      "---------------------------------------------------------\n",
      "32.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "32.0 K    Total params\n",
      "0.128     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4aea3a8881f42138a3eea13cd1dbca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=20, accelerator='auto')\n",
    "trainer.fit(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c849348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL WITH DROPOUT TRAINING\n",
    "\n",
    "class MNISTExperiment(pl.LightningModule):\n",
    "    def __init__(self, learning_rate: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.model = ModelWithDropout(dropout_prob=0.1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.train_scores = torchmetrics.Accuracy(compute_on_step=False, task='multiclass', num_classes=10)  # Placeholder for training scores\n",
    "        self.validation_scores = torchmetrics.Accuracy(compute_on_step=False, task='multiclass', num_classes=10)   # Placeholder for validation scores\n",
    "        self.test_scores = torchmetrics.Accuracy(compute_on_step=False, task='multiclass', num_classes=10)   # Placeholder for test scores\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        self.train_scores(preds, y)\n",
    "\n",
    "        self.log('train_accuracy', self.train_scores, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx) -> None:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        self.validation_scores(preds, y)\n",
    "\n",
    "    def test_step(self, batch, batch_idx) -> None:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        self.test_scores(preds, y)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset = MNIST('data/', train=True, download=True, transform=transform)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "        return dataloader\n",
    "\n",
    "experiment2 = MNISTExperiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c775b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses = PlotLossesCallback()\n",
    "\n",
    "# Define the loggers\n",
    "csv_logger = CSVLogger('logs', name='mnist_logs')\n",
    "tensorboard_logger = TensorBoardLogger('logs', name='mnist_logs')\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator='auto',\n",
    "    logger=[csv_logger, tensorboard_logger],\n",
    "    callbacks=[early_stopping, plot_losses]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d021addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(max_epochs=20, accelerator='auto')\n",
    "trainer.fit(experiment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80b6d31c",
   "metadata": {},
   "source": [
    "## 5) Conclusion (1 Point)\n",
    "Comment on your findings:\n",
    "- Show the results in a table via Pandas\n",
    "- Which method is better? Why?\n",
    "- Are the results significant? If not, how can we get significant ones?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73fea8c6",
   "metadata": {},
   "source": [
    "I had problems with training eventhough I had a powerful machine. I believe the versions did not match so pytorch could not work well with my machine. \n",
    "\n",
    "Bath normalization would speed up the process since it normalizes the inputs of each layer by scaling the activations and dropout temporarily drops out randomly chosen neurons which helps the neural network to overcome overfitting problem and enable the model to adjust better. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
