{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zWsI04UdxEju"
      },
      "source": [
        "## Today, you are becoming a classical music prodigy!\n",
        "\n",
        "You will learn how to generate classical piano music in the style of different composers like Mozart, Beethoven, and Bach using deep learning. This is an exciting opportunity to use cutting-edge technology to create your own unique pieces of music. \n",
        "![Famous Classiscal piano composers](https://www.pianostreet.com/blog/wp-content/uploads/2023/01/12-piano-composers-1.jpg) \n",
        "Source: https://www.pianostreet.com/blog/site-news/the-complete-piano-works-of-12-composers-12132/\n",
        "1. Collect MIDI files for classical piano music pieces written by several different composers. We will provide with the data for this project, but you can also collect your own data if you want to experiment with other composers or genres of music. Therefore download the data from https://www.kaggle.com/datasets/soumikrakshit/classical-music-midi and store it in a folder called \"data\". The subfolders in the \"data\" folder should be named after the composers whose music you want to use for training the model. \n",
        "![Midi Data Format Represenation: Music is not stored as sound but rather in a format similar to sheet music](https://developer-blogs.nvidia.com/wp-content/uploads/2022/08/image9-3.png) \n",
        "Source: https://developer.nvidia.com/blog/leveraging-ai-music-with-nvidia-dgx-2/                                                                                                                                       \n",
        "2. Use a Python library called \"music21\" to extract the notes and timing information from your MIDI files. This information will be used as inputs for your neural network.\n",
        "3. Use PyTorch to build a neural network model that can classify composers from sequences of notes. You will then use the same model architecture to learn the next notes in a piece of music and generate classical piano music in the style of different composers.\n",
        "4. Your neural network model will consist of an LSTM (Long Short-Term Memory) layer followed by two fully connected layers. You will train the model on your MIDI data and evaluate its performance by generating new pieces of music in the style of the different composers. In case you do not have a GPU you can also load the pretrained mode *best_model.pth* by putting it into your root directory of the notebook.\n",
        "\n",
        "**To generate new music:**\n",
        "\n",
        "1. Use a technique called \"sampling\" where you feed a seed sequence of notes into the trained model, which then generates a new sequence of notes based on its learned patterns. You will repeat this process multiple times to generate longer pieces of music.\n",
        "2. Use the music21 library to convert the generated sequences of notes back into MIDI files, which can be played back using a digital audio workstation or music software. The resulting output can be used as inspiration for new compositions or as a tool for music education and analysis.\n",
        "\n",
        "This is a challenging but rewarding project. I am confident that you will be able to generate beautiful and inspiring music using deep learning. So let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "79-qEOqFxEjz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define a list of required packages\n",
        "packages = {\n",
        "    \"music21\": \"7.3.3\",\n",
        "    \"numpy\": \"1.24.2\",\n",
        "    \"pygame\": \"2.4.0\",\n",
        "    \"torch\": \"2.0.0\",\n",
        "    \"matplotlib\": \"3.7.1\",\n",
        "}\n",
        "\n",
        "\n",
        "# Install packages# Install packages with specified versions\n",
        "# for package, version in packages.items():\n",
        "#     !pip install {package}=={version}\n",
        "\n",
        "# # Restart the kernel to apply the changes\n",
        "# import IPython\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0WUFTZ76xEj2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.4.0 (SDL 2.26.4, Python 3.9.13)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Any\n",
        "\n",
        "import music21\n",
        "import numpy as np\n",
        "import pygame\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "from torch.cuda.amp import autocast\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cU-1nUYaxEj3"
      },
      "source": [
        "# Data Collection\n",
        "In the next step we are collecting the MIDI files for the different composers. We are using the music21 library to load the MIDI files for the different composers. We are using the following composers for this project:\n",
        "Albeniz, Bach, Balakirev, Beethoven, Borodin, Brahms, BurgmÃ¼ller, Chopin, Debussy, Granados, Grieg, Haydn, Liszt, Mendelssohn, Mozart, Mussorgsky, Schubert, Schumann, Tchaikovsky\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfYJXwoXxEj4",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "# Here we define the datasets for the different composers\n",
        "\n",
        "\n",
        "\n",
        "class MidiDataset(Dataset):\n",
        "    \"\"\"A dataset of MIDI files.\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): The features of the dataset.\n",
        "        y (np.ndarray): The note labels of the dataset.\n",
        "        composer_name (str): The name of the composer.\n",
        "\n",
        "    Attributes:\n",
        "        X (np.ndarray): The features of the dataset.\n",
        "        y (np.ndarray): The note labels of the dataset.\n",
        "        composer_name (str): The name of the composer.\n",
        "        num_samples (int): The number of samples in the dataset.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray, composer_name: str):\n",
        "        \"\"\"Initialize the dataset.\"\"\"\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.composer_name = composer_name\n",
        "        self.num_samples = len(self.X)\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Return the features and labels for the given index.\"\"\"\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Here we define the datasets for the different composers and merge them into a single dataset for training and testing the model on all the composers together. We also print the length of the corpus and plot the histogram of notes. We can see that the corpus is highly imbalanced with a few notes being very common and most notes being very rare. This is a common problem in music or language and we will deal with it later in the project.\n",
        "\n",
        "class MultiComposerPianoMidiDataset(Dataset):\n",
        "    \"\"\"A dataset of MIDI files from multiple composers.\n",
        "\n",
        "    Args:\n",
        "        root_dir (Path): The path to the root directory of the dataset.\n",
        "        num_most_common_notes (int): The number of most common notes to use.\n",
        "\n",
        "\n",
        "    Attributes:\n",
        "        root_dir (Path): The path to the root directory of the dataset.\n",
        "        all_datasets (List[MidiDataset]): A list of all datasets for each composer.\n",
        "        num_note_classes (int): The number of classes in the dataset.\n",
        "\n",
        "\n",
        "        most_common_notes (List[str]): The most common notes in the dataset.\n",
        "        note_to_label (Dict[str, int]): A mapping from note to label.\n",
        "        label_to_note (Dict[int, str]): A mapping from label to note.\n",
        "        composer_to_label (Dict[str, int]): A mapping from composer to label.\n",
        "        label_to_composer (Dict[int, str]): A mapping from label to composer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            root_dir: Path,\n",
        "            num_most_common_notes: int=100,\n",
        "            seq_len: int=40,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize the dataset.\"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.seq_len = seq_len\n",
        "        self.all_datasets = []\n",
        "\n",
        "        # Count occurrences of all notes in all composer notes\n",
        "        all_notes = []\n",
        "        load_npy = False\n",
        "        print(\"Time to grab a coffee. Loading and cleaning the data the first time takes a while (20 minutes). However, you can already start answering the questions! The second time it will be much faster.\")\n",
        "        for composer_dir in self.root_dir.iterdir():\n",
        "\n",
        "            if not composer_dir.is_dir():\n",
        "                continue\n",
        "            if composer_dir.name.startswith('.'):\n",
        "                continue\n",
        "            if (composer_dir / \"X.npy\").is_file():\n",
        "                load_npy = True\n",
        "                # Load dataset from saved numpy files\n",
        "                composer_dataset = MidiDataset(\n",
        "                    X=np.load(composer_dir / \"X.npy\"),\n",
        "                    y=np.load(composer_dir / \"y.npy\"),\n",
        "                    composer_name=composer_dir.name\n",
        "                )\n",
        "                print(f\"Composer {composer_dir.name} has {len(composer_dataset)} sequences/samples\")\n",
        "                for note in composer_dataset.y :\n",
        "                    all_notes.append(int(note))\n",
        "\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"Loading composer data from {composer_dir.name} for data set inspection\")\n",
        "                all_midis = []\n",
        "                for midi_file in composer_dir.glob(\"*.mid\"):\n",
        "                    midi = music21.converter.parse(midi_file)\n",
        "                    all_midis.append(midi)\n",
        "\n",
        "                all_notes.extend(self.extract_notes(all_midis))\n",
        "        # plot the histogram, note that just if you create the dataset you actually observe also the rare notes, if you load the dataset from the saved numpy files you only observe the most common notes\n",
        "        if not load_npy:\n",
        "\n",
        "            self.plot_histogram_of_notes(all_notes)\n",
        "            count_num = Counter(all_notes)\n",
        "            self.most_common_notes = [note for note, count in count_num.most_common(num_most_common_notes)]\n",
        "\n",
        "            # Create a mapping from note to label\n",
        "            self.note_to_label = {note: label for label, note in enumerate(self.most_common_notes)}\n",
        "            self.label_to_note = {label: note for note, label in self.note_to_label.items()}\n",
        "            self.num_note_classes = len(self.note_to_label)\n",
        "\n",
        "            # Create a mapping from composer to label\n",
        "            self.composer_to_label = {composer_dir.name: label for label, composer_dir in enumerate(self.root_dir.iterdir())}\n",
        "            self.label_to_composer = {label: composer for composer, label in self.composer_to_label.items()}\n",
        "            self.num_composer_classes = len(self.composer_to_label)\n",
        "\n",
        "            # save to json file\n",
        "            self.save_json_files()\n",
        "        else:\n",
        "            # load from json file\n",
        "            self.load_json_files()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Iterate over all the composer folders in root_dir\n",
        "        for i,composer_dir in enumerate(self.root_dir.iterdir()):\n",
        "            print(f\"{composer_dir} {i}\")\n",
        "            if not composer_dir.is_dir():\n",
        "                continue\n",
        "            if composer_dir.name.startswith('.'):\n",
        "                continue\n",
        "            if (composer_dir / \"X.npy\").is_file():\n",
        "                print(f\"Loading composer data from {composer_dir.name}\")\n",
        "                # Load dataset from saved numpy files\n",
        "                composer_dataset = MidiDataset(\n",
        "                    X=np.load(composer_dir / \"X.npy\"),\n",
        "                    y=np.load(composer_dir / \"y.npy\"),\n",
        "                    composer_name=composer_dir.name\n",
        "                )\n",
        "                self.all_datasets.append(composer_dataset)\n",
        "                continue\n",
        "            else:\n",
        "                # Create a dataset for each composer\n",
        "                print(f\"Creating dataset for composer {composer_dir.name}\")\n",
        "                composer_dataset = self.create_dataset(composer_dir, self.most_common_notes)\n",
        "                self.all_datasets.append(composer_dataset)\n",
        "\n",
        "        # Merge all the composer datasets into a single dataset\n",
        "        self.X = np.concatenate([dataset.X for dataset in self.all_datasets], axis=0)\n",
        "        self.y_notes = np.concatenate([dataset.y for dataset in self.all_datasets], axis=0)\n",
        "\n",
        "\n",
        "        # Map a list of composers to labels\n",
        "        self.composer_list = [dataset.composer_name for dataset in self.all_datasets]\n",
        "        self.y_composer = np.concatenate(\n",
        "            [[self.composer_to_label[dataset.composer_name]] * dataset.__len__() for  dataset in self.all_datasets],\n",
        "            axis=0)[..., np.newaxis]\n",
        "\n",
        "\n",
        "\n",
        "        # Shuffle the data while preserving the correspondence between inputs and labels\n",
        "        # this is  important because we split the data into train and test later and we want to have a good distribution of composers in both sets\n",
        "        indices = np.random.permutation(len(self.X))\n",
        "        self.X = self.X[indices]\n",
        "        self.y_notes = self.y_notes[indices]\n",
        "        self.y_composer = self.y_composer[indices]\n",
        "\n",
        "\n",
        "\n",
        "    def create_dataset(self, composer_dir: Path, most_common_notes: List[str]) -> MidiDataset:\n",
        "        \"\"\"Create a new dataset from MIDI files in `composer_dir`.\n",
        "\n",
        "        Args:\n",
        "            composer_dir (Path): The path to the composer directory.\n",
        "            most_common_notes (List[str]): The most common notes to include in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            A `MidiDataset` object.\n",
        "        \"\"\"\n",
        "\n",
        "        # Create new dataset from MIDI files in composer_dir\n",
        "        all_midis = []\n",
        "        for midi_file in composer_dir.glob(\"*.mid\"):\n",
        "            midi = music21.converter.parse(midi_file)\n",
        "            all_midis.append(midi)\n",
        "\n",
        "        # Extract notes from all MIDI files\n",
        "        notes = self.extract_notes(all_midis)\n",
        "\n",
        "\n",
        "        # Select the most common notes\n",
        "        selected_notes = [note for note in notes if note in most_common_notes]\n",
        "\n",
        "        # Map each note to a label\n",
        "        selected_notes = [self.note_to_label[note] for note in selected_notes]\n",
        "\n",
        "\n",
        "        # Create a list of features and targets\n",
        "        features = []\n",
        "        targets = []\n",
        "\n",
        "        for i in range(self.seq_len,len(selected_notes)-1):\n",
        "            feature = selected_notes[i-self.seq_len+1:i + 1]\n",
        "            target = selected_notes[i + 1]\n",
        "            features.append(feature)\n",
        "            targets.append(target)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Convert the features and targets to NumPy arrays\n",
        "        features = np.array(features)\n",
        "        targets = np.array(targets)\n",
        "\n",
        "\n",
        "        # Save the dataset to a file\n",
        "        self.save_to_np(composer_dir,(features, targets))\n",
        "\n",
        "        # Create a `MidiDataset` object\n",
        "        dataset = MidiDataset(features, targets, composer_dir.name)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "\n",
        "    def extract_notes(self,all_midis: List[music21.stream.Score]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extract notes from all MIDI files.\n",
        "\n",
        "        Args:\n",
        "            all_midis (List[music21.stream.Score]): A list of MIDI files.\n",
        "\n",
        "        Returns:\n",
        "            A list of notes.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        notes = []\n",
        "        pick = None\n",
        "        for midi in all_midis:\n",
        "            songs = midi.parts.stream()\n",
        "            for part in songs:\n",
        "                pick = part.recurse()\n",
        "                for element in pick:\n",
        "                    if isinstance(element, int):\n",
        "                        notes.append(music21.note.Note.pitch.midiToNoteName(element))\n",
        "                    elif isinstance(element, music21.note.Note):\n",
        "                        if element.pitch.name:  # Skip empty notes\n",
        "                            notes.append(str(element.pitch))\n",
        "                    elif isinstance(element, music21.chord.Chord):\n",
        "                        chord_notes = [str(n) for n in element.pitches if n.name]  # Skip empty notes\n",
        "                        if chord_notes:\n",
        "                            notes.append(\".\".join(chord_notes))\n",
        "        print(f\"Number of notes extracted: {len(notes)}\")\n",
        "        return notes\n",
        "\n",
        "    def plot_histogram_of_notes(self, notes: List[Any]) -> None:\n",
        "        \"\"\"Plot a histogram of notes.\n",
        "\n",
        "        Args:\n",
        "            notes (List[str]): A list of notes.\n",
        "        \"\"\"\n",
        "        print(\"Note that the histogram is only displayed the first time you load the data. Afterwards the data is not laoded from the .mid files, but directly from clean .np-files that contain only the most frequent notes.\")\n",
        "\n",
        "        note_counts = Counter(notes)\n",
        "        sorted_note_counts = sorted(note_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "        sorted_frequencies = [count for note, count in sorted_note_counts]\n",
        "        num_notes = len(sorted_frequencies)\n",
        "\n",
        "        bins = num_notes // 20\n",
        "\n",
        "        plt.hist(sorted_frequencies, bins=bins)\n",
        "        plt.xlabel(\"Number of Notes\")\n",
        "        plt.ylabel(\"Frequency (#Counts)\")\n",
        "        plt.title(\"Histogram of Notes\")\n",
        "        plt.xscale('log')\n",
        "        plt.yscale('log')\n",
        "        plt.xticks(np.logspace(0, 4, num=5), rotation=45)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.show()\n",
        "        plt.savefig(\"histogram_of_notes.png\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Return the number of samples in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            The number of samples in the dataset.\n",
        "        \"\"\"\n",
        "\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"Return the features, target notes, and composer labels for the given index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The index of the sample to return.\n",
        "\n",
        "        Returns:\n",
        "            A tuple of features, target notes, and composer labels.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.X[idx], self.y_notes[idx], self.y_composer[idx]\n",
        "    def save_json_files(self):\n",
        "        \"\"\"Saves all the json files in the self.root_dir of the data.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Save the note to label mapping.\n",
        "        with open( \"note_to_label.json\", \"w\") as fp:\n",
        "            json.dump(self.note_to_label, fp)\n",
        "\n",
        "        # Save the label to note mapping.\n",
        "        with open(\"label_to_note.json\", \"w\") as fp:\n",
        "            json.dump(self.label_to_note, fp)\n",
        "\n",
        "        # Save the composer to label mapping.\n",
        "        with open(\"composer_to_label.json\", \"w\") as fp:\n",
        "            json.dump(self.composer_to_label, fp)\n",
        "\n",
        "        # Save the label to composer mapping.\n",
        "        with open( \"label_to_composer.json\", \"w\") as fp:\n",
        "            json.dump(self.label_to_composer, fp)\n",
        "\n",
        "        # Save the most common notes.\n",
        "        with open(\"most_common_notes.json\", \"w\") as fp:\n",
        "            json.dump(self.most_common_notes, fp)\n",
        "\n",
        "    def load_json_files(self):\n",
        "        \"\"\"Loads all the json files in the self.root_dir of the data.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Load the note to label mapping.\n",
        "        with open( \"note_to_label.json\", \"r\") as fp:\n",
        "            self.note_to_label = json.load(fp)\n",
        "\n",
        "        # Load the label to note mapping.\n",
        "        with open(\"label_to_note.json\", \"r\") as fp:\n",
        "            self.label_to_note = json.load(fp)\n",
        "\n",
        "        # Load the composer to label mapping.\n",
        "        with open( \"composer_to_label.json\", \"r\") as fp:\n",
        "            self.composer_to_label = json.load(fp)\n",
        "\n",
        "        # Load the label to composer mapping.\n",
        "        with open(\"label_to_composer.json\", \"r\") as fp:\n",
        "            self.label_to_composer = json.load(fp)\n",
        "\n",
        "        # Load the most common notes.\n",
        "        with open( \"most_common_notes.json\", \"r\") as fp:\n",
        "            self.most_common_notes = json.load(fp)\n",
        "\n",
        "        self.num_note_classes = len(self.note_to_label)\n",
        "        self.num_composer_classes = len(self.composer_to_label)\n",
        "\n",
        "\n",
        "        # Convert keys to integers (they are strings when loaded from json)\n",
        "        self.label_to_note= {int(label): note for label, note in self.label_to_note.items()}\n",
        "        self.note_to_label = {note: int(label) for note, label in self.note_to_label.items()}\n",
        "        self.label_to_composer = {int(label): composer for label, composer in self.label_to_composer.items()}\n",
        "        self.composer_to_label = {composer: int(label) for composer, label in self.composer_to_label.items()}\n",
        "\n",
        "\n",
        "\n",
        "    def save_to_np(self, componist_folder: Path, data: Tuple[np.ndarray, np.ndarray]) -> None:\n",
        "        \"\"\"Save the dataset to NumPy files.\n",
        "\n",
        "        Args:\n",
        "            componist_folder (Path): The path to the composer directory.\n",
        "            data (Tuple[np.ndarray, np.ndarray]): A tuple of features and target notes.\n",
        "        \"\"\"\n",
        "\n",
        "        path_X = componist_folder / \"X.npy\"\n",
        "        path_y = componist_folder / \"y.npy\"\n",
        "        np.save(path_X, data[0])\n",
        "        np.save(path_y, data[1])\n",
        "\n",
        "    def load_from_np(self, componist_folder: Path) -> None:\n",
        "        \"\"\"Load the dataset from NumPy files.\n",
        "\n",
        "        Args:\n",
        "            componist_folder (Path): The path to the composer directory.\n",
        "        \"\"\"\n",
        "\n",
        "        path_X = componist_folder / \"X.npy\"\n",
        "        path_y = componist_folder / \"y.npy\"\n",
        "        self.X = np.load(path_X)\n",
        "        self.y = np.load(path_y)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWuZ1iV8xEj6",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# here we define the collate function that is used to combine the data samples into a batch for the dataloader\n",
        "# the collate function yields a tuple of the inputs and labels for each batch. the inputs a sequence of notes and the labels are a dict of the next note in the sequence and the composer label\n",
        "\n",
        "\n",
        "def collate_fn_notes_componist_labels(batch: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Collates a batch of data for training or evaluation.\n",
        "\n",
        "    Args:\n",
        "        batch: A list of tuples of (inputs, labels_notes, labels_componist).\n",
        "\n",
        "    Returns:\n",
        "        A tuple of (inputs, labels).\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the inputs and labels from the batch.\n",
        "    inputs, labels_notes, labels_componist = zip(*batch)\n",
        "\n",
        "    # Convert inputs and labels to float tensors.\n",
        "    inputs = [torch.from_numpy(x).float() for x in inputs]\n",
        "    labels_notes = [float(y) for y in labels_notes]\n",
        "    labels_componist = [torch.from_numpy(y).float() for y in labels_componist]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Stack inputs and labels into tensors.\n",
        "    inputs = torch.stack(inputs).unsqueeze(2)  # add channel dimension\n",
        "    labels_notes = torch.tensor(labels_notes)\n",
        "    labels_componist = torch.tensor(labels_componist)\n",
        "    labels = {\"notes\": labels_notes, \"composers\": labels_componist}\n",
        "\n",
        "    return inputs, labels\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1C3ubdWxEj7",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "# here we define the LSTM model that we will use to predict the next note in the sequence and the composer of the sequence of notes (componist classification)\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    \"\"\"A long short-term memory (LSTM) model.\n",
        "\n",
        "    Args:\n",
        "        input_size: The size of the input tensor.\n",
        "        hidden_size: The size of the hidden state.\n",
        "        num_layers: The number of LSTM layers.\n",
        "        num_notes: The number of output classes for the note classification task.\n",
        "        num_componists: The number of output classes for the composer classification task.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, num_notes: int, num_componists: int) -> None:\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        # Parameters\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.num_notes = num_notes\n",
        "        self.num_componists = num_componists\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Linear layers\n",
        "        self.ln = nn.LayerNorm(hidden_size)\n",
        "        self.fc11 = nn.Linear(hidden_size, num_notes)\n",
        "        self.fc12 = nn.Linear(hidden_size, num_componists)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "        # ReLU\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Initialize weights\n",
        "        for name, param in self.lstm.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                torch.nn.init.xavier_normal_(param)\n",
        "            elif 'bias' in name:\n",
        "                torch.nn.init.constant_(param, 0.0)\n",
        "\n",
        "        # Initialize biases of linear layers to 0\n",
        "\n",
        "        torch.nn.init.constant_(self.fc11.bias, 0.0)\n",
        "        torch.nn.init.constant_(self.fc12.bias, 0.0)\n",
        "\n",
        "        # Initialize weights of linear layers with Xavier initialization\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.fc11.weight)\n",
        "        torch.nn.init.xavier_normal_(self.fc12.weight)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Forward pass of the LSTM model.\n",
        "\n",
        "        Args:\n",
        "            x: The input tensor.\n",
        "\n",
        "        Returns:\n",
        "            The output tensors for the note classification task and the composer classification task.\n",
        "        \"\"\"\n",
        "\n",
        "        # Set the initial hidden state to 0\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward pass through the LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Apply ReLU\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # Normalize the output\n",
        "        out = self.ln(out)\n",
        "\n",
        "        # Apply dropout\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # Get the output for the note classification task\n",
        "        out_notes = self.fc11(out[:, -1, :])\n",
        "\n",
        "        # Get the output for the composer classification task\n",
        "        out_composer = self.fc12(out[:, -1, :])\n",
        "\n",
        "        return out_notes, out_composer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5EKlJapxEj8",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "# here a function for creating the dataset and yielding the dataset, samplers and num labels is defined. Note that usually one would try to split sequential data into instances that are not overlapping. For music this owuld mean that one would split the training and test data into non-overlapping sequences of notes, i.e. splitting the pieces or even using different pieces for training and testing. However, for the sake of simplicity we will not do this here.\n",
        "\n",
        "def create_dataset(root_dir: Path) -> Tuple[MultiComposerPianoMidiDataset,SubsetRandomSampler,SubsetRandomSampler, int, int]:\n",
        "    \"\"\"\n",
        "    Creates a dataset of classical piano music.\n",
        "\n",
        "    Args:\n",
        "        root_dir: The path to the directory where the dataset is stored.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the dataset and the number of classes and composers.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create dataset.\n",
        "    dataset = MultiComposerPianoMidiDataset(root_dir=root_dir)\n",
        "\n",
        "\n",
        "\n",
        "    # Create a sampler that samples the first 95% of the data.\n",
        "    train_sampler = SubsetRandomSampler(list(range(len(dataset)*19 // 20)))\n",
        "\n",
        "    # Create a sampler that samples the remaining 5% of the data.\n",
        "    test_sampler = SubsetRandomSampler(list(range(len(dataset)*19 // 20, len(dataset))))\n",
        "\n",
        "    num_classes = dataset.num_note_classes\n",
        "    num_composers = dataset.num_composer_classes\n",
        "\n",
        "    return dataset,train_sampler,test_sampler, num_classes, num_composers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1bpotEVxEj8"
      },
      "outputs": [],
      "source": [
        "# here we define the function that we will use to load the data. it should take the dataset, the batch size and the samplers as input and return the dataloaders\n",
        "\n",
        "def create_dataloader(\n",
        "        dataset: MultiComposerPianoMidiDataset,\n",
        "        train_sampler: SubsetRandomSampler,\n",
        "        test_sampler: SubsetRandomSampler,\n",
        "        batch_size: int,\n",
        "        num_workers: int = 8,\n",
        "\n",
        ") -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n",
        "    \"\"\"\n",
        "    Creates dataloaders for the dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset: The dataset.\n",
        "        train_sampler: The sampler for the training data.\n",
        "        test_sampler: The sampler for the validation data.\n",
        "        batch_size: The batch size to use.\n",
        "        num_workers: The number of workers to use for loading the data.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the train and validation dataloaders.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create data loaders.\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset, sampler=train_sampler,batch_size=batch_size,  collate_fn=collate_fn_notes_componist_labels, num_workers=num_workers\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        dataset, sampler=test_sampler,batch_size=batch_size,  collate_fn=collate_fn_notes_componist_labels\n",
        "        , num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsJxrrgxxEj9",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "# here we define the validation function that we will use to evaluate the model on the validation set after each epoch of training. it should take logits and labels as input and return the accuracy\n",
        "\n",
        "def calculate_metric(outputs: torch.Tensor, targets: torch.Tensor) -> float:\n",
        "    \"\"\"Calculates the accuracy of the model on the given outputs and targets.\n",
        "\n",
        "    Args:\n",
        "        outputs: The model's outputs.\n",
        "        targets: The ground truth labels.\n",
        "\n",
        "    Returns:\n",
        "        The accuracy of the model.\n",
        "    \"\"\"\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total = targets.size(0)\n",
        "    correct = (predicted == targets).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def visualize_validation_accuracy(val_accuracies: List[float],label_str: str) -> None:\n",
        "    \"\"\"Plots the validation accuracy.\n",
        "\n",
        "    Args:\n",
        "        val_accuracies: The validation accuracies.\n",
        "\n",
        "    Returns:\n",
        "        None.\n",
        "    \"\"\"\n",
        "    plt.close('all')\n",
        "    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies)\n",
        "    plt.title('Validation Accuracy '+label_str)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeG6ALMwxEj-"
      },
      "outputs": [],
      "source": [
        "# a function for validation after each epoch is defined here\n",
        "def validation_after_epoch(\n",
        "        model: torch.nn.Module,\n",
        "        val_loader: torch.utils.data.DataLoader,\n",
        "        best_val_loss: float,\n",
        "        val_accuracies_note: List[float],\n",
        "        val_accuracies_composer: List[float],\n",
        "\n",
        "        device: torch.device,\n",
        ") -> float:\n",
        "    \"\"\"Validates the model after each epoch.\n",
        "\n",
        "    Args:\n",
        "        model: The model to validate.\n",
        "        val_loader: The dataloader for the validation set.\n",
        "        best_val_loss: The best validation accuracy so far.\n",
        "        val_accuracies_note: A list to store the validation accuracies for the note classification task.\n",
        "        val_accuracies_composer: A list to store the validation accuracies for the composer classification task.\n",
        "\n",
        "    Returns:\n",
        "        None.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set the model to evaluation mode.\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Validate the model.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct_note, correct_composer = 0, 0\n",
        "        total_note, total_composer = 0, 0\n",
        "        loss_note, loss_composer = 0, 0\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.float().to(device)\n",
        "\n",
        "            prediction_note, prediction_composer = model(inputs)\n",
        "            _, predicted_note = torch.max(prediction_note, 1)\n",
        "            _, predicted_composer = torch.max(prediction_composer, 1)\n",
        "\n",
        "            total_note += labels[\"notes\"].size(0)\n",
        "            correct_note += (predicted_note == labels[\"notes\"].to(device)).sum().item()\n",
        "            total_composer += labels[\"composers\"].size(0)\n",
        "            correct_composer += (predicted_composer == labels[\"composers\"].to(device)).sum().item()\n",
        "\n",
        "            loss_note += torch.nn.functional.cross_entropy(prediction_note, labels[\"notes\"].to(device).long(), reduction=\"mean\").item()\n",
        "            loss_composer += torch.nn.functional.cross_entropy(prediction_composer, labels[\"composers\"].to(device).long(), reduction=\"mean\").item()\n",
        "\n",
        "\n",
        "\n",
        "        accuracy_note = 100 * correct_note / total_note\n",
        "        accuracy_composer = 100 * correct_composer / total_composer\n",
        "        loss_note /= len(val_loader)\n",
        "        loss_composer /= len(val_loader)\n",
        "        total_val_loss=loss_note+loss_composer\n",
        "        val_accuracies_note.append(accuracy_note)\n",
        "        val_accuracies_composer.append(accuracy_composer)\n",
        "        print(f\"Validation Accuracy Note: {accuracy_note:.2f}%\")\n",
        "        print(f\"Validation Accuracy Composer: {accuracy_composer:.2f}%\")\n",
        "        print(f\"Validation Loss: {total_val_loss:.2f}\")\n",
        "        print(f\"Validation Loss Composer: {loss_composer:.2f}\")\n",
        "        print(f\"Validation Loss Note: {loss_note:.2f}\")\n",
        "\n",
        "        # plot the validation accuracy\n",
        "        # visualize_validation_accuracy(val_accuracies_note,\"Note\")\n",
        "        # visualize_validation_accuracy(val_accuracies_composer,\"Composer\")\n",
        "        # Save model if validation accuracy has improved.\n",
        "\n",
        "\n",
        "        if total_val_loss < best_val_loss:\n",
        "            print(\"New best model found!\\n Saving model...\")\n",
        "            best_val_loss = total_val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return best_val_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JONaCRh7xEj_"
      },
      "outputs": [],
      "source": [
        "# here we define the training function that we will use to train the model. it should take the model, the dataloaders, the optimizer, the loss function (criterion) and the number of epochs as input and return the trained model and the training history (loss and accuracy on the training and validation set)\n",
        "\n",
        "\n",
        "def train(\n",
        "        input_size: int,\n",
        "        hidden_size: int,\n",
        "        num_layers: int,\n",
        "        num_epochs: int,\n",
        "        learning_rate: float,\n",
        "        train_loader: torch.utils.data.DataLoader,\n",
        "        val_loader: torch.utils.data.DataLoader,\n",
        "        num_classes: int,\n",
        "        num_composers: int,\n",
        "        device: torch.device,\n",
        "        model_path:Path,\n",
        "        fine_tune: bool = False,\n",
        "        parallel: bool = False,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Trains a model on the given dataset.\n",
        "\n",
        "    Args:\n",
        "        input_size: The size of the input vector.\n",
        "        hidden_size: The size of the hidden layer.\n",
        "        num_layers: The number of layers in the model.\n",
        "        num_epochs: The number of epochs to train for.\n",
        "        learning_rate: The learning rate to use.\n",
        "        train_loader: The dataloader for the training set.\n",
        "        val_loader: The dataloader for the validation set.\n",
        "        num_classes: The number of classes in the dataset.\n",
        "        num_composers: The number of composers in the dataset.\n",
        "        device: The device to train the model on.\n",
        "\n",
        "    Returns:\n",
        "        None.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if device == \"cuda\" and parallel:\n",
        "        model = nn.DataParallel(LSTM(input_size, hidden_size, num_layers, num_classes, num_composers)).to(device)\n",
        "    else:\n",
        "        model = LSTM(input_size, hidden_size, num_layers, num_classes, num_composers).to(device)\n",
        "    if fine_tune:\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        learning_rate = learning_rate / 10\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "    # Train model\n",
        "    train_losses = []\n",
        "    val_accuracies_note, val_accuracies_composer = [], []\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs = inputs.float().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                # Forward pass\n",
        "                prediction_note, prediction_composer = model(inputs)\n",
        "\n",
        "                loss = criterion(prediction_note, labels[\"notes\"].long().to(device))\n",
        "                loss += criterion(prediction_composer, labels[\"composers\"].long().to(device))\n",
        "\n",
        "            # Backward and optimize\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(\n",
        "                    f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "            train_losses.append(loss.item())\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step(running_loss)\n",
        "        # Validate model after each epoch\n",
        "        best_val_loss=validation_after_epoch(model, val_loader, best_val_loss, val_accuracies_note, val_accuracies_composer,device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYusZ5MfxEj_"
      },
      "outputs": [],
      "source": [
        "# this function is for loading the model after the training is done. it should take the model, the path to the model and the device as input and return the model. if you can't train the LSTM model on your machine, you can use the pretrained model that we have provided for you  \"best_model.pth\"\n",
        "\n",
        "\n",
        "def load_and_initialize_model(\n",
        "        model_path: Path,\n",
        "        num_composers: int,\n",
        "        num_classes: int,\n",
        "        device: torch.device,\n",
        "        input_size: int = 128,\n",
        "        hidden_size: int = 128,\n",
        "        num_layers: int = 1,\n",
        "\n",
        ") -> nn.Module:\n",
        "    \"\"\"Loads and initializes a model.\n",
        "\n",
        "    Args:\n",
        "        model_path: The path to the model checkpoint.\n",
        "        num_composers: The number of composers in the dataset.\n",
        "        num_classes: The number of classes in the dataset.\n",
        "        input_size: The size of the input vector.\n",
        "        hidden_size: The size of the hidden layer.\n",
        "        num_layers: The number of layers in the model.\n",
        "        device: The device to load the model on.\n",
        "\n",
        "    Returns:\n",
        "        The loaded and initialized model.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize the model.\n",
        "    model = LSTM(input_size, hidden_size, num_layers, num_classes, num_composers).to(device)\n",
        "\n",
        "    # Load the model weights into the model\n",
        "    model.load_state_dict(torch.load(model_path,map_location=device))\n",
        "\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmA4XKXJxEkA"
      },
      "outputs": [],
      "source": [
        "# this function is for testing the model. it should take the model, the test dataloader and the device as input and return the accuracy on the test set.\n",
        "\n",
        "\n",
        "\n",
        "def calculate_metrics(normalized_confusion_matrix: torch.Tensor) -> Tuple[float, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Calculate accuracy, precision, and recall metrics from a confusion matrix.\n",
        "\n",
        "    Args:\n",
        "        normalized_confusion_matrix (torch.Tensor): The confusion matrix.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, torch.Tensor, torch.Tensor]: A tuple containing accuracy, precision, and recall.\n",
        "    \"\"\"\n",
        "    # Calculate accuracy\n",
        "    accuracy = torch.diagonal(normalized_confusion_matrix).sum().item() / normalized_confusion_matrix.sum().item()\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    num_classes = normalized_confusion_matrix.size(0)\n",
        "    precision = torch.zeros(num_classes)\n",
        "    recall = torch.zeros(num_classes)\n",
        "    for i in range(num_classes):\n",
        "        true_positives = normalized_confusion_matrix[i, i].item()\n",
        "        false_positives = normalized_confusion_matrix[:, i].sum().item() - true_positives\n",
        "        false_negatives = normalized_confusion_matrix[i, :].sum().item() - true_positives\n",
        "\n",
        "        precision[i] = true_positives / (true_positives + false_positives + 1e-8)\n",
        "        recall[i] = true_positives / (true_positives + false_negatives + 1e-8)\n",
        "\n",
        "    return accuracy, precision, recall\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(confusion_matrix, composer_to_label, label_to_composer,title):\n",
        "    \"\"\"\n",
        "    Plot a confusion matrix.\n",
        "\n",
        "    Args:\n",
        "        confusion_matrix (torch.Tensor): The confusion matrix.\n",
        "        class_labels (List[str]): The class labels.\n",
        "        title (str): The title of the plot.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Create a figure and axis\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(confusion_matrix, cmap='Blues')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.xticks(ticks=np.arange(len(composer_to_label)),\n",
        "               labels=[composer_to_label[i] for i in\n",
        "                       range(-1, len(label_to_composer) - 1)], rotation=90)\n",
        "    plt.yticks(ticks=np.arange(len(composer_to_label)),\n",
        "               labels=[composer_to_label[i] for i in\n",
        "                       range(-1, len(label_to_composer) - 1)])\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "    plt.savefig(title + '.png')\n",
        "\n",
        "\n",
        "def confusion_matrices(model: torch.nn.Module, test_loader: DataLoader, threshold: float):\n",
        "    \"\"\"\n",
        "    Calculate confusion matrices for composer and note predictions.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The trained model.\n",
        "        test_loader (torch.utils.data.DataLoader): DataLoader for the test dataset.\n",
        "        threshold (float): The threshold for binary classification.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Confusion matrix for composer predictions.\n",
        "        torch.Tensor: Confusion matrix for note predictions.\n",
        "    \"\"\"\n",
        "    # Initialize confusion matrices\n",
        "    num_composer_classes = test_loader.dataset.num_composer_classes\n",
        "    num_note_classes = test_loader.dataset.num_note_classes\n",
        "    confusion_matrix_composer = torch.zeros(num_composer_classes, num_composer_classes)\n",
        "    confusion_matrix_note = torch.zeros(num_note_classes, num_note_classes)\n",
        "    confusion_matrix_composer_argmax = torch.zeros(num_composer_classes, num_composer_classes)\n",
        "    confusion_matrix_note_argmax = torch.zeros(num_note_classes, num_note_classes)\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Iterate over the test dataset\n",
        "    for x, y in test_loader:\n",
        "        x = x\n",
        "        y_note, y_composer = y[\"notes\"], y[\"composers\"]\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            prediction_note, prediction_composer = model(x)\n",
        "\n",
        "        # Apply threshold to predictions\n",
        "        y_hat_note = (prediction_note.sigmoid() > threshold).float()\n",
        "        y_hat_composer = (prediction_composer.sigmoid() > threshold).float()\n",
        "        y_hat_note_argmax = prediction_note.argmax(dim=1)\n",
        "        y_hat_composer_argmax = prediction_composer.argmax(dim=1)\n",
        "\n",
        "        # One-hot encode targets\n",
        "        y_note = torch.nn.functional.one_hot(y_note.long(), num_classes=num_note_classes).float()\n",
        "        y_composer = torch.nn.functional.one_hot(y_composer.long(), num_classes=num_composer_classes).float()\n",
        "\n",
        "        # Update note confusion matrix\n",
        "        confusion_matrix_note = torch.add(confusion_matrix_note, torch.matmul(y_note.t(), y_hat_note))\n",
        "\n",
        "        # Update composer confusion matrix\n",
        "        confusion_matrix_composer = torch.add(confusion_matrix_composer, torch.matmul(y_composer.t(), y_hat_composer))\n",
        "\n",
        "        # Update note confusion matrix based on argmax\n",
        "        confusion_matrix_note_argmax += torch.nn.functional.one_hot(y_hat_note_argmax, num_classes=num_note_classes).float().t() @ y_note\n",
        "\n",
        "        # Update composer confusion matrix based on argmax\n",
        "        confusion_matrix_composer_argmax += torch.nn.functional.one_hot(y_hat_composer_argmax, num_classes=num_composer_classes).float().t() @ y_composer\n",
        "\n",
        "    # Normalize confusion matrices\n",
        "    normalized_confusion_matrix_composer = confusion_matrix_composer / confusion_matrix_composer.sum(dim=1, keepdim=True)\n",
        "    normalized_confusion_matrix_note = confusion_matrix_note / confusion_matrix_note.sum(dim=1, keepdim=True)\n",
        "    normalized_confusion_matrix_composer_argmax = confusion_matrix_composer_argmax / confusion_matrix_composer_argmax.sum(dim=1, keepdim=True)\n",
        "    normalized_confusion_matrix_note_argmax = confusion_matrix_note_argmax / confusion_matrix_note_argmax.sum(dim=1, keepdim=True)\n",
        "    plot_list=[normalized_confusion_matrix_composer,normalized_confusion_matrix_composer_argmax]\n",
        "    if threshold == 0.5:\n",
        "        for index, confusion_matrix in enumerate(plot_list):\n",
        "            if index == 0:\n",
        "                title = 'confusion_matrix_composer_treshold_0.5'\n",
        "            else:\n",
        "                title = 'confusion_matrix_composer_argmax'\n",
        "\n",
        "            # Plot the confusion matrix for the composer predictions\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            plt.imshow(confusion_matrix, cmap='Blues')\n",
        "            plt.title(title)\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('True')\n",
        "            plt.xticks(ticks=np.arange(len(test_loader.dataset.composer_to_label)),\n",
        "                       labels=[test_loader.dataset.label_to_composer[i] for i in\n",
        "                               range( len(test_loader.dataset.label_to_composer) )], rotation=90)\n",
        "            plt.yticks(ticks=np.arange(len(test_loader.dataset.composer_to_label)),\n",
        "                       labels=[test_loader.dataset.label_to_composer[i] for i in\n",
        "                               range( len(test_loader.dataset.label_to_composer) )])\n",
        "\n",
        "            # Annotate the diagonal elements\n",
        "            for i in range(len(test_loader.dataset.composer_to_label)):\n",
        "                for j in range(len(test_loader.dataset.composer_to_label)):\n",
        "                    value = \"{:.2f}\".format(confusion_matrix[i, j])\n",
        "                    plt.text(j, i, value, ha='center', va='center', color='white' if i == j else 'black')\n",
        "\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "            plt.savefig(title + '.png')\n",
        "\n",
        "    return normalized_confusion_matrix_composer, normalized_confusion_matrix_note, normalized_confusion_matrix_composer_argmax, normalized_confusion_matrix_note_argmax\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "        model: torch.nn.Module,\n",
        "        test_loader: DataLoader,\n",
        "        thresholds: List[float]\n",
        ") -> List[Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Evaluate the model using different thresholds and calculate performance metrics.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The trained model to evaluate.\n",
        "        test_loader (DataLoader): The data loader for the test dataset.\n",
        "        thresholds (List[float]): List of thresholds to use for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, float]]: List of dictionaries containing performance metrics for each threshold.\n",
        "            Each dictionary includes the following metrics:\n",
        "                - 'threshold': The threshold used for evaluation.\n",
        "                - 'composer_accuracy': Accuracy of composer predictions.\n",
        "                - 'composer_precision': Precision of composer predictions.\n",
        "                - 'composer_recall': Recall of composer predictions.\n",
        "                - 'note_accuracy': Accuracy of note predictions.\n",
        "                - 'note_precision': Precision of note predictions.\n",
        "                - 'note_recall': Recall of note predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set model to evaluation mode and move it to CPU\n",
        "    model.eval().cpu()\n",
        "\n",
        "    # List to store performance metrics for different thresholds\n",
        "    performance_metrics = []\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        print(f\"Evaluating model with threshold {threshold:.2f}...\")\n",
        "\n",
        "        # Compute confusion matrices for composer and note predictions\n",
        "        confusion_matrix_composer, confusion_matrix_note, confusion_matrix_composer_argmax,confusion_matrix_note_argmax = confusion_matrices(model, test_loader, threshold)\n",
        "\n",
        "        # Calculate metrics for composer predictions\n",
        "        # Calculate metrics for composer predictions\n",
        "        composer_accuracy, composer_precision, composer_recall = calculate_metrics(confusion_matrix_composer)\n",
        "        if threshold == 0.5:\n",
        "            composer_accuracy_argmax, composer_precision_argmax, composer_recall_argmax = calculate_metrics(\n",
        "                confusion_matrix_composer_argmax)\n",
        "            print(f\"composer_accuracy_argmax: {composer_accuracy_argmax}\")\n",
        "            print(f\"composer_precision_argmax: {composer_precision_argmax}\")\n",
        "            print(f\"composer_recall_argmax: {composer_recall_argmax}\")\n",
        "\n",
        "        # Calculate metrics for note predictions\n",
        "        note_accuracy, note_precision, note_recall = calculate_metrics(confusion_matrix_note)\n",
        "\n",
        "        # Store performance metrics for the current threshold\n",
        "        performance_metrics.append({\n",
        "            'threshold': threshold,\n",
        "            'composer_accuracy': composer_accuracy,\n",
        "            'composer_precision': composer_precision,\n",
        "            'composer_recall': composer_recall,\n",
        "            'note_accuracy': note_accuracy,\n",
        "            'note_precision': note_precision,\n",
        "            'note_recall': note_recall,\n",
        "        })\n",
        "\n",
        "    return performance_metrics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_metrics(\n",
        "        thresholds: List[float],\n",
        "        accuracy_scores: List[float],\n",
        "        precision_scores: List[float],\n",
        "        recall_scores: List[float],\n",
        "        title: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Plot accuracy, precision, and recall for different thresholds.\n",
        "\n",
        "    Args:\n",
        "        thresholds (List[float]): List of threshold values.\n",
        "        accuracy_scores (List[float]): List of accuracy scores.\n",
        "        precision_scores (List[float]): List of precision scores.\n",
        "        recall_scores (List[float]): List of recall scores.\n",
        "        title (str): Title of the plot.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(thresholds, accuracy_scores, label='Accuracy', linestyle='-', marker='o')\n",
        "    plt.plot(thresholds, precision_scores, label='Precision', linestyle='--', marker='s')\n",
        "    plt.plot(thresholds, recall_scores, label='Recall', linestyle=':', marker='^')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Threshold')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calculate_and_visualize_metrics(\n",
        "        model: torch.nn.Module,\n",
        "        test_loader: DataLoader,\n",
        "        thresholds: List[float]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Calculate and visualize the performance metrics for composer and note predictions.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The trained model.\n",
        "        test_loader (DataLoader): The data loader for the test dataset.\n",
        "        thresholds (List[float]): List of thresholds used for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Evaluate the model and obtain performance metrics\n",
        "    performance_metrics = evaluate_model(model, test_loader, thresholds)\n",
        "\n",
        "    # Extract the thresholds\n",
        "    thresholds = [metric['threshold'] for metric in performance_metrics]\n",
        "\n",
        "    # Extract composer scores for each metric\n",
        "    accuracy_scores = [metric['composer_accuracy'] for metric in performance_metrics]\n",
        "    precision_scores = [metric['composer_precision'].mean().item() for metric in performance_metrics]\n",
        "    recall_scores = [metric['composer_recall'].mean().item() for metric in performance_metrics]\n",
        "\n",
        "    # Plot the composer prediction metrics\n",
        "    plot_metrics(thresholds, accuracy_scores, precision_scores, recall_scores, title='Composer Prediction Metrics')\n",
        "\n",
        "    # Extract note scores for each metric\n",
        "    accuracy_scores = [metric['note_accuracy'] for metric in performance_metrics]\n",
        "    precision_scores = [metric['note_precision'].mean().item() for metric in performance_metrics]\n",
        "    recall_scores = [metric['note_recall'].mean().item() for metric in performance_metrics]\n",
        "\n",
        "    # Plot the note prediction metrics\n",
        "    plot_metrics(thresholds, accuracy_scores, precision_scores, recall_scores, title='Note Prediction Metrics')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYf1aCqJxEkC"
      },
      "outputs": [],
      "source": [
        "# this function is for creating your own music with the trained model.\n",
        "\n",
        "def play_midi(midi_file):\n",
        "    \"\"\"\n",
        "    Play a MIDI file using Pygame.\n",
        "    :param midi_file:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Initialize Pygame\n",
        "    pygame.init()\n",
        "\n",
        "    # Load the MIDI file\n",
        "    pygame.mixer.music.load(midi_file)\n",
        "\n",
        "    # Play the MIDI file\n",
        "    pygame.mixer.music.play()\n",
        "\n",
        "    # Wait until the music finishes playing\n",
        "    while pygame.mixer.music.get_busy():\n",
        "        pygame.time.Clock().tick(10)\n",
        "\n",
        "    # Quit Pygame\n",
        "    pygame.quit()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_classical_piano_music(\n",
        "    model: torch.nn.Module,\n",
        "    test_loader: torch.utils.data.DataLoader,\n",
        "    file_path: Path,\n",
        "    seed: int = 0,\n",
        "    output_length: int = 10,\n",
        "        musescore: bool = False\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Generates classical piano music using a trained model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The trained model.\n",
        "        test_loader (torch.utils.data.DataLoader): The data loader.\n",
        "        file_path (Path): The file path to save the generated music.\n",
        "        output_length (int): Length of the output sequence to generate. Default is 100.\n",
        "\n",
        "    Returns:\n",
        "        None.\n",
        "    \"\"\"\n",
        "\n",
        "    # Select a random sequence from the validation set to start with.\n",
        "    validation_set = test_loader.dataset\n",
        "    np.random.seed(seed)\n",
        "    start_sequence, _, composer = validation_set[np.random.randint(len(validation_set))]\n",
        "    print(f\"Start sequence is from composer {test_loader.dataset.label_to_composer[composer.item()]}\")\n",
        "    sequence_length = start_sequence.shape[0]\n",
        "\n",
        "    # Convert the start sequence to notes.\n",
        "    notes = start_sequence.flatten().tolist()\n",
        "\n",
        "    model.cpu()\n",
        "    # Generate music.\n",
        "    for i in range(output_length):\n",
        "        # Get the input to the model.\n",
        "        inputs = torch.tensor(notes[-sequence_length:], dtype=torch.float32).view(1, -1, 1).cpu()\n",
        "\n",
        "        # Get the output of the model.\n",
        "        note_pred, composer_pred = model(inputs)\n",
        "\n",
        "        # Get the probabilities of each note.\n",
        "        probabilities = torch.softmax(note_pred, dim=1).squeeze().detach().numpy()\n",
        "\n",
        "        # Choose a note randomly according to the probabilities.\n",
        "        index = np.random.choice(len(probabilities), p=probabilities)\n",
        "\n",
        "        # Add the note to the list of notes.\n",
        "        notes.append(index)\n",
        "\n",
        "    # Convert labels to notes\n",
        "    notes = [test_loader.dataset.label_to_note[note] for note in notes]\n",
        "    print(f\"Generated sequence of notes by your neural network: {notes}\")\n",
        "    # Create a MIDI stream.\n",
        "    midi_stream = music21.stream.Stream()\n",
        "\n",
        "    # Add the notes to the MIDI stream.\n",
        "    for note in notes:\n",
        "        if '.' in note:\n",
        "            chord_notes = [n for n in note.split('.') if n]  # Skip empty strings\n",
        "            chord_obj = music21.chord.Chord(chord_notes)\n",
        "            midi_stream.append(chord_obj)\n",
        "        else:\n",
        "            note_obj = music21.note.Note(note)\n",
        "            midi_stream.append(note_obj)\n",
        "\n",
        "    # Write the MIDI stream to a file.\n",
        "    midi_file_path = Path(file_path).resolve()  # Get the absolute path of the file\n",
        "    midi_file_path.parent.mkdir(parents=True, exist_ok=True)  # Create the parent directory if it doesn't exist\n",
        "    midi_stream.write('midi', fp=str(midi_file_path))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Show the MIDI stream if musescore is installed .\n",
        "    if musescore:\n",
        "\n",
        "        # play the music\n",
        "        play_midi(midi_file_path)\n",
        "        midi_stream.show()\n",
        "\n",
        "        # Convert the MIDI file to a WAV file.\n",
        "        wav_file_path = midi_file_path.with_suffix('.wav')\n",
        "        music21.converter.parse(str(midi_file_path)).write('wav', fp=str(wav_file_path))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6EyUkvsAxEkC"
      },
      "source": [
        "# Summary of the `train` function\n",
        "\n",
        "The `train` function trains a model to classify notes and composers in piano MIDI music. The function first creates a dataset of MIDI files and splits it into a training set and a validation set. It then creates data loaders for the training and validation sets. Next, the function creates a model and loads the state of the model from a file. It then defines a loss function and an optimizer. The function then trains the model for a specified number of epochs. After each epoch, the function validates the model and saves the model if the validation accuracy has improved. The function returns the training losses and the validation accuracies.\n",
        "\n",
        "## Steps of the `train` function\n",
        "\n",
        "1. Create a dataset of MIDI files and split it into a training set and a validation set.\n",
        "2. Create data loaders for the training and validation sets.\n",
        "3. Create a model and load the state of the model from a file if `fine_tune` is `True`.\n",
        "4. Define a loss function and an optimizer.\n",
        "5. Train the model for a specified number of epochs. IMPORTANT: If you can't train the model set TRAIN to False and load the pretrained model and skip the training steps 6 and 7.\n",
        "6. After each epoch, validate the model and save the model if the validation accuracy has improved.\n",
        "7. Return the training losses and the validation accuracies.\n",
        "\n",
        "\n",
        "## Steps of the `calculate_and_visualize_metrics` function\n",
        "1. Evaluate the model on the test dataset and obtain performance metrics for different thresholds.\n",
        "2. Extract the thresholds used for evaluation from the performance metrics.\n",
        "3. Extract the accuracy, precision, and recall scores for composer predictions.\n",
        "4. Plot the composer prediction metrics, including accuracy, precision, and recall, for different thresholds.\n",
        "5. Extract the accuracy, precision, and recall scores for note predictions.\n",
        "6. Plot the note prediction metrics, including accuracy, precision, and recall, for different thresholds.\n",
        "\n",
        "## Create your own classical piano music with the `generate_your_own_classical_piano_music` function\n",
        "1. Generate music.\n",
        "2. Create a MIDI stream.\n",
        "3. Add the notes to the MIDI stream.\n",
        "4. Write the MIDI stream to a file.\n",
        "5. Show the MIDI stream.\n",
        "\n",
        "\n",
        "## Exercise Questions\n",
        "\n",
        "The idea of this exercise is not to do a lot of programming but rather trying to understand the code and the concepts behind it. Therefore, you don't have to write any code for this exercise. \n",
        "\n",
        "1. Why do you think we want to use RNN/LSTM for this task and not, for example, a CNN or a fully connected network?\n",
        "\n",
        "2. What is the difference between the training and validation set?\n",
        "\n",
        "3. Could you think of other metrics that we could use to evaluate the model?\n",
        "\n",
        "4. Could you think of other ways to improve the model?\n",
        "\n",
        "5. Could we use a different loss function for this task? If yes, which one and why?\n",
        "\n",
        "6. How long are the sequences that we use to train the model?\n",
        "\n",
        "7. Do you think that the model will perform better if we use longer sequences? If yes, why?\n",
        "\n",
        "8. Are RNNs hard to train? If yes, why?\n",
        "\n",
        "9. What are the benefits of using LSTMs over RNNs?\n",
        "\n",
        "10. What is the difference when performing backpropagation in RNNs and LSTMs compared to fully connected networks?\n",
        "\n",
        "11. Why might it take quite a long time to train the LSTM model compared to the fully connected network?\n",
        "\n",
        "12. Why might there be a difference in performance between the composer and note predictions?\n",
        "\n",
        "13. What might be a better way to split the dataset into training and validation sets?\n",
        "\n",
        "14. Why is the performance in the confusion matrix for a fixed threshold of 0.5 different from the confusion matrix if we use argmax to predict the class?\n",
        "\n",
        "15. What might this tell us about the model?\n",
        "\n",
        "16. If there is a strong off-diagonal component in the confusion matrix of the composers, what might this tell us about the data or the composers?\n",
        "\n",
        "17. The LSTM model is trained using a batch size of 64. What impact does the batch size have on the training process? How would the training process be affected if we increased or decreased the batch size?\n",
        "\n",
        "18. In the code, we use dropout regularization with a rate of 0.1. What is the purpose of dropout regularization? How does it help prevent overfitting? Are there any potential drawbacks or limitations of using dropout?\n",
        "\n",
        "19. The code includes a step where the model's predictions are post-processed using a threshold value. What considerations should be taken into account when choosing an appropriate threshold value? How might the choice of threshold affect the model's performance and the balance between precision and recall?\n",
        "\n",
        "20. In the code, we evaluate the model's performance using accuracy as a metric. Discuss the limitations of using accuracy as the sole evaluation metric for this classification task. Are there any other evaluation metrics that could provide a more comprehensive assessment of the model's performance?\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TSrC3X587qZw"
      },
      "source": [
        "## Please enter your answer here:\n",
        "1. RNN's and LSTM's are preferred because these neurons are able to capture temporal dependencies and can capture the trend of the dataset.These networks are better for sequential data because they have internal memory cells so that they can use information from previous steps. \n",
        "2. Training dataset is used to train the neural network. Validation dataset is used to compare how accurate the model is by comparing the neural net to the validation dataset on each epoch.\n",
        "3. Since music is a subjective topic, although it would sound funny, human evaluation would be a good metric.\n",
        "4. Adding more songs and more composers would improve the results because the model would not only learn the approach of a particular composer, it would also be able to capture the foundations of music theory.\n",
        "5. A GAN could also be trained. The loss function in GANs usually able to capture reconstruction losses with discriminator. \n",
        "6. Seq Length is given as 40.\n",
        "7. Choosing a sensible sequence length is tricky because it needs to be able to give a sensible trend without including too much randomness. In music, the flow is usually around 4 bars. So 4 bars would be a sensible measure.\n",
        "8. RNN's are hard to train compared to ANN and CNN because of vanishing/exploding gradients during backpropagation. Also, it takes the data in batches and requires information from previous step. Finally, choosing a right batch size is crucial not to overfit a RNN model.\n",
        "9. A LSTM is a more complex neuron which has gates inside it. It is able to store forget and update information through these gates. Also, it can handle different lenghts of sequences.\n",
        "10.\n",
        "11.\n",
        "12.\n",
        "13.\n",
        "14.\n",
        "15.\n",
        "16.\n",
        "17.\n",
        "18.\n",
        "19.\n",
        "20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrlnaVNVxEkD"
      },
      "outputs": [],
      "source": [
        "# here we set the hyperparameters for the training and testing of the model.\n",
        "# Hyperparameters\n",
        "INPUT_SIZE = 1\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_LAYERS = 2\n",
        "NUM_WORKERS=16\n",
        "BATCH_SIZE =  64\n",
        "LEARNING_RATE = .001\n",
        "NUM_EPOCHS = 1000\n",
        "SEQUENCE_LENGTH = 40\n",
        "\n",
        "# To generate another music sequence change the seed, e.g. 1,2,3,4...\n",
        "RANDOM_SEED_4_MUSIC_GEN = 0\n",
        "\n",
        "# Enter the path of your \"data\" folder \n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/data\")\n",
        "MODEL_PATH=Path(\"best_model.pth\")\n",
        "TRAIN_MODEL = False\n",
        "FINE_TUNE_MODEL = False\n",
        "PARALLEL= True\n",
        "MUSE_SCORE = False\n",
        "MUSIC_FILE_PATH = Path('generated_music.mid')\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwrvfoMlzm7U"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyaCdpE0xEkE"
      },
      "outputs": [],
      "source": [
        "# this is the main function that will be called when we run the script.\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    This function is the main entry point for the program.\n",
        "\n",
        "    Returns:\n",
        "      None.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    # Create dataset.\n",
        "    dataset, train_sampler,test_sampler, num_classes, num_composers = create_dataset(root_dir=DATA_DIR)\n",
        "\n",
        "    # Create dataloaders.\n",
        "    train_loader, val_loader = create_dataloader(dataset=dataset, train_sampler=train_sampler,\n",
        "                                                 test_sampler=test_sampler, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "\n",
        "\n",
        "    # Train the model if `TRAIN_MODEL` is True.\n",
        "    if TRAIN_MODEL:\n",
        "\n",
        "\n",
        "        # Train the model.\n",
        "        train(train_loader=train_loader,\n",
        "                      val_loader=val_loader,\n",
        "                      num_classes=num_classes,\n",
        "                      num_composers=num_composers,\n",
        "                      input_size=INPUT_SIZE,\n",
        "                      hidden_size=HIDDEN_SIZE,\n",
        "                      num_layers=NUM_LAYERS,\n",
        "                      learning_rate=LEARNING_RATE,\n",
        "                      num_epochs=NUM_EPOCHS,\n",
        "                      device=DEVICE,\n",
        "                    model_path=MODEL_PATH,\n",
        "                      fine_tune=FINE_TUNE_MODEL,\n",
        "                      parallel=PARALLEL,\n",
        "                      )\n",
        "\n",
        "    # Load the trained model.\n",
        "    model = load_and_initialize_model(model_path=MODEL_PATH,\n",
        "                                      num_composers=num_composers,\n",
        "                                      num_classes=num_classes,\n",
        "                                      input_size=INPUT_SIZE,\n",
        "                                      hidden_size=HIDDEN_SIZE,\n",
        "                                      num_layers=NUM_LAYERS,\n",
        "                                      device=DEVICE,)\n",
        "\n",
        "    # Evaluate the model.\n",
        "    calculate_and_visualize_metrics(model=model,\n",
        "                                    test_loader=val_loader,\n",
        "                                    thresholds=[0.1,0.5,0.9,0.95,0.99,0.999],\n",
        "                                    )\n",
        "\n",
        "\n",
        "\n",
        "    # Generate your own classical piano music.\n",
        "    # if you want to visualize the generated music, set `musescore` to True and make sure you have musescore installed.\n",
        "    # Otherwise, set `musescore` to False. To install MuseScore on your system, you can visit the MuseScore website (https://musescore.org/) and download the appropriate installer for your operating system. Once installed, you can try running the mscore3 command again to verify its installation.\n",
        "    generate_classical_piano_music(model=model,\n",
        "                                            test_loader=val_loader,file_path=MUSIC_FILE_PATH,seed=RANDOM_SEED_4_MUSIC_GEN,musescore=MUSE_SCORE,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f35UblzgxEkE"
      },
      "outputs": [],
      "source": [
        "# this cell runs the main function.\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
